{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Convolution2D, Flatten, Dense, Input, add\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.merge import Add, Average\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "ENV_NAME = 'SpaceInvaders-v0'  # Environment name\n",
    "FRAME_WIDTH = 84  # Resized frame width\n",
    "FRAME_HEIGHT = 84  # Resized frame height\n",
    "NUM_EPISODES = 12000  # Number of episodes the agent plays\n",
    "STATE_LENGTH = 4  # Number of most recent frames to produce the input to the network\n",
    "GAMMA = 0.99  # Discount factor\n",
    "EXPLORATION_STEPS = 1000000  # Number of steps over which the initial value of epsilon is linearly annealed to its final value\n",
    "# EXPLORATION_STEPS = 5  # Number of steps over which the initial value of epsilon is linearly annealed to its final value\n",
    "INITIAL_EPSILON = 1.0  # Initial value of epsilon in epsilon-greedy\n",
    "FINAL_EPSILON = 0.1  # Final value of epsilon in epsilon-greedy\n",
    "INITIAL_REPLAY_SIZE = 50000  # Number of steps to populate the replay memory before training starts\n",
    "NUM_REPLAY_MEMORY = 1000000  # Number of replay memory the agent uses for training\n",
    "BATCH_SIZE = 32  # Mini batch size\n",
    "TARGET_UPDATE_INTERVAL = 10000  # The frequency with which the target network is updated\n",
    "TRAIN_INTERVAL = 4  # The agent selects 4 actions between successive updates\n",
    "LEARNING_RATE = 0.0001  # Learning rate used by ADAM\n",
    "SAVE_INTERVAL = 50000  # The frequency with which the network is saved\n",
    "NO_OP_STEPS = 30  # Maximum number of \"do nothing\" actions to be performed by the agent at the start of an episode\n",
    "LOAD_NETWORK = False\n",
    "TRAIN = True\n",
    "SAVE_NETWORK_PATH = 'saved_networks/' + ENV_NAME\n",
    "SAVE_SUMMARY_PATH = 'summary/' + ENV_NAME\n",
    "NUM_EPISODES_AT_TEST = 20  # Number of episodes the agent plays at test time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_actions = 3\n",
    "\n",
    "c = np.array([[[1,2, 0],[3,4, 0]],[[3,4, 0],[1,1,1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.52163333]\n",
      " [ 0.51647991]]\n",
      "[[ 0.15054417 -2.31784558 -2.40236092]\n",
      " [-0.26557648 -2.75055599 -3.22334433]]\n",
      "[[ 0.67217749 -1.7962122  -1.88072753]\n",
      " [ 0.25090343 -2.23407602 -2.70686436]]\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "s = Input(shape=(2,3))\n",
    "f = Flatten()(s)\n",
    "dense_value = Dense(4, activation='relu')(f)\n",
    "dense_advantage = Dense(4, activation='relu')(f)\n",
    "dense_value_out = Dense(1)(dense_value)\n",
    "dense_advantage_out = Dense(3)(dense_advantage)\n",
    "# q = Lambda(lambda s,y:s+y-K.mean(y), arguments={'y':dense_advantage_out})(dense_value_out)\n",
    "q = Lambda(lambda s,y:s+y, arguments={'y':dense_advantage_out})(dense_value_out)\n",
    "\n",
    "model = Model(inputs=s, outputs=q)\n",
    "\n",
    "q_vals = model(s)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "dv =  sess.run(dense_value_out,feed_dict={s:c})\n",
    "da =  sess.run(dense_advantage_out,feed_dict={s:c})\n",
    "q =  sess.run(q_vals,feed_dict={s:c})\n",
    "print dv\n",
    "print da\n",
    "print q\n",
    "print \"===\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.52163333]\n",
      " [ 0.51647991]]\n",
      "[[ 0.15054417 -2.31784558 -2.40236092]\n",
      " [-0.26557648 -2.75055599 -3.22334433]]\n",
      "[[ 0.67217749 -1.7962122  -1.88072753]\n",
      " [ 0.25090343 -2.23407602 -2.70686436]]\n"
     ]
    }
   ],
   "source": [
    "# added = tf.add(da,dv)\n",
    "print dv\n",
    "print da\n",
    "added = dv + da \n",
    "print added\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
